#from bs4 import BeautifulSoup
#import requests
#import re
#import os
#
#BASE_URL = "https://www.vulnhub.com"
#PAGE_QUERY = BASE_URL + "/?page=" 
#
#class MachineInfo():
#    def __str__(self):
#        result = ""
#        result += "Machine name: " + self.name + "\n"
#        result += "Date release: " + self.date + "\n"
#        result += "Author: " + self.author + "\n"
#        result += "Series: " + self.series + "\n"
#        return result
#
#class VulnHubMachine():
#    def __init__(self, info, download, description, url):
#        self.info = info
#        self.download = download
#        self.description = description
#        self.url = BASE_URL + url
#
#    def __str__(self):
#        result = str(self.info)
#        result += "Download: " + self.download + "\n"
#        result += "Url: " + self.url + "\n"
#        result += "Description: " + self.description + "\n"
#        return result
#
#
#def get_pages():
#    result = list()
#    req = requests.get(BASE_URL)
#    soup = BeautifulSoup(req.text,features="lxml")
#
#    list_div = soup.findAll("div", {"class": "text-center pagination"})
#    for d in list_div:
#        pages = d.findAll("a", href=re.compile("page"), text=re.compile("[0-9]+"))
#    for p in pages:
#        result.append(PAGE_QUERY + p.text)
#    return result
#
#def get_info_by_type(req, info_type):
#    info = ""
#    soup = BeautifulSoup(req.text,features="lxml")
#    info_div = soup.find("div", {"id": "release"})
#    try:
#        info = info_div.find("b", text=re.compile(info_type)).find_parent("li")
#        info = info.text.split(info_type+": ")[1]
#    except AttributeError:
#        info = ""
#    return info
#
#def get_info(req):
#    info_types = ['Name', 'Date release', 'Author', 'Series']
#    machine_info = MachineInfo()
#    for t in info_types:
#        setattr(machine_info, t.lower().split()[0], get_info_by_type(req, t))
#    return machine_info
#
#def get_download(req):
#    soup = BeautifulSoup(req.text,features="lxml")
#    download_div = soup.find("div", {"id": "download"})
#    try:
#        download = download_div.find("b", text=re.compile("Download \(Mirror\)")).find_parent("li")
#    except AttributeError:
#        pass
#    try:
#        result = download.text.split("Download (Mirror): ")[1]
#    except Exception:
#        result = ""
#    return result
#
#def get_description(req):
#    soup = BeautifulSoup(req.text,features="lxml")
#    description_div = soup.find("div", {"id": "description"})
#    try:
#        description = description_div.find("p").text
#    except AttributeError:
#        description = ""
#    return description
#
#def get_machine(entry_url):
#    MACHINE_URL = BASE_URL + entry_url
#    req = requests.get(MACHINE_URL)
#    info = get_info(req)
#    download = get_download(req)
#    description = get_description(req)
#    return VulnHubMachine(info, download, description, entry_url)
#
#
#
#
#def get_machines_per_page(page):
#    result = list()
#    req = requests.get(page)
#    soup = BeautifulSoup(req.text,features="lxml")
#
#    list_div = soup.findAll("div", {"class": "span12 entry"})
#    for d in list_div:
#        entry_url = d.find("div", {"class": "span9 entry-title"}).find("a").attrs['href'] 
#        m = get_machine(entry_url)
#        result.append(m)
#        print(m)
#    return result
#
#
#
#
##pages = get_pages()
##machines = list()
##
##for p in pages:
##    machines += get_machines_per_page(p)
##
##print("Scraped machines: "+ str(len(machines)))
